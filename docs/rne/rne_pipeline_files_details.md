# rne_pipeline_files_details.md

## 1. Objectif
Ce document fournit une vision complÃ¨te, technique et fonctionnelle, de lâ€™ensemble du pipeline RNE.  
Il dÃ©taille les DAGs Airflow, les fichiers exÃ©cutÃ©s, les fonctions, les modÃ¨les et les sÃ©quences dâ€™appel, mais aussi la logique mÃ©tier sous-jacente (pourquoi chaque Ã©tape existe, ce quâ€™elle produit, ce quâ€™elle consomme).  

Lâ€™objectif est que mÃªme un non-dÃ©veloppeur puisse comprendre :
- ce qui se passe rÃ©ellement dans le pipeline,
- dans quel ordre,
- quels fichiers sont impliquÃ©s,
- et comment la base consolidÃ©e RNE est produite chaque jour.

Le document reprend la partie 5 du fichier `2.1_RNE_Architecture.md` et lâ€™Ã©tend en une vue opÃ©rationnelle dÃ©taillÃ©e.

---

## 2. Vue dâ€™ensemble du pipeline RNE

### 2.1 Vue simple (pour non-dÃ©veloppeur)

STOCK (run manuel une seule fois)
    â†“ Ã©crit dans MinIO rne/stock/

FLUX (tous les jours Ã  01h00)
    â†“ Ã©crit dans MinIO rne/flux/

BASE CONSOLIDÃ‰E RNE (tous les jours Ã  02h00)
    â†“ lit rne/stock + rne/flux (sauf flux J)
    â†“ gÃ©nÃ¨re rne_<date>.db.gz
    â†“ met Ã  jour latest_rne_date.json

### 2.2 Vue technique
DAGs Airflow impliquÃ©s :
- **get_rne_stock**  
  Rapatrie le ZIP du stock et dÃ©pose les JSON en `rne/stock/`.

- **get_flux_rne**  
  TÃ©lÃ©charge les flux RNE Diff depuis lâ€™API INPI, un fichier `.json.gz` par jour, stockÃ©s en `rne/flux/`.

- **fill_rne_database**  
  Construit une base SQLite consolidÃ©e contenant tout lâ€™historique : stock + flux jusquâ€™Ã  Jâ€“1.

Stockage centralisÃ© :
- MinIO est la source unique de vÃ©ritÃ© :
  - `rne/stock/`
  - `rne/flux/`
  - `rne/database/`

Rythme :
- Stock : ponctuel.
- Flux : quotidien.
- Consolidation : quotidienne (exclut J par sÃ©curitÃ©).

---

## 3. Glossaire (important pour non-dev)

**DAG** : un workflow Airflow composÃ© de tÃ¢ches ordonnÃ©es.  
**Task** : une Ã©tape individuelle dâ€™un DAG.  
**MinIO** : stockage objet compatible S3.  
**Stock RNE** : photographie complÃ¨te du registre INPI Ã  un instant T.  
**Flux RNE** : mises Ã  jour quotidiennes du registre.  
**Snapshot SQLite** : base gÃ©nÃ©rÃ©e chaque jour pour exploitation interne.  
**XCom** : mÃ©canisme Airflow pour transmettre des donnÃ©es entre tÃ¢ches.  
**Flux J** : flux du jour en cours, ignorÃ© par sÃ©curitÃ© (souvent incomplet).  
**First run** : premiÃ¨re exÃ©cution historique (inclut le stock).  

---

## 4. Logique fonctionnelle du pipeline (la raison de chaque Ã©tape)

### 4.1 Pourquoi un stock ?
Il donne un **Ã©tat initial complet** du registre RNE.  
Sans ce point de dÃ©part, impossible de reconstruire un historique fiable.

### 4.2 Pourquoi des flux ?
Ils amÃ¨nent **tous les changements quotidiens** :
- crÃ©ations,
- radiations,
- modifications,
- dirigeants,
- adresses,
- activitÃ©s.

### 4.3 Pourquoi une base consolidÃ©e ?
Les JSON RNE sont bruts, volumineux et difficiles Ã  manipuler.  
La consolidation produit :
- une structure tabulaire exploitable,
- une vue unifiÃ©e,
- un rÃ©fÃ©rentiel interne fiable,
- une base versionnÃ©e par jour.

### 4.4 Pourquoi ignorer le flux J ?
Le flux du jour peut Ãªtre :
- incomplet,
- en cours de constitution,
- corrompu lors dâ€™une rÃ©cupÃ©ration partielle.

Pour garantir la fiabilitÃ© :
â†’ le pipeline intÃ¨gre les flux **jusquâ€™Ã  Jâ€“1**.

### 4.5 Pourquoi des rÃ©pertoires temporaires ?
Pour Ã©viter que des fichiers partiellement Ã©crits ou corrompus nâ€™influencent les runs suivants.

---

## 5. DÃ©tail complet par Ã©tape (partie 5 enrichie)

### 5.1 Acquisition du stock RNE
- **DAG :** `get_rne_stock`  
  (`workflows/data_pipelines/rne/stock/DAG.py`)

#### Ordre des tÃ¢ches
1. `clean_previous_outputs`  
2. `get_rne_latest_stock`  
3. `unzip_files_and_upload_minio`

#### Fichiers utilisÃ©s
- `workflows/data_pipelines/rne/stock/DAG.py`
- `workflows/data_pipelines/rne/stock/processor.py`
- `workflows/data_pipelines/rne/stock/config.py`
- `workflows/data_pipelines/rne/stock/get_stock.sh`

#### Fonctions / classes clÃ©s
- `RneStockProcessor` : orchestre les Ã©tapes de tÃ©lÃ©chargement et dâ€™upload.
- `download_stock` : exÃ©cute `get_stock.sh` et tÃ©lÃ©charge le ZIP du stock RNE.
- `send_stock_to_minio` : lit le ZIP, extrait chaque JSON et lâ€™envoie vers MinIO.

#### Fonctionnellement
- TÃ©lÃ©charge le ZIP officiel INPI.
- Extrait tous les JSON.
- Envoie chaque fichier dans `rne/stock/`.
- Cette Ã©tape nâ€™est normalement exÃ©cutÃ©e quâ€™une seule fois (ou lors dâ€™une rÃ©initialisation).

---

### 5.2 Acquisition quotidienne des flux RNE
- **DAG :** `get_flux_rne`  
  (`workflows/data_pipelines/rne/flux/DAG.py`)

#### Ordre des tÃ¢ches
1. `clean_previous_outputs`  
2. `get_every_day_flux`  
3. `clean_outputs`  
4. `send_notification_success_mattermost` (en cas dâ€™Ã©chec, callback `send_notification_failure_mattermost`)

#### Fichiers utilisÃ©s
- `workflows/data_pipelines/rne/flux/flux_tasks.py`
- `workflows/data_pipelines/rne/flux/rne_api.py`

#### Fonctions / classes clÃ©s
- `compute_start_date` : Inspecte les fichiers flux dÃ©jÃ  prÃ©sents dans MinIO et renvoie **la derniÃ¨re date trouvÃ©e**, pas le jour suivant. La boucle `get_every_day_flux` redÃ©marre donc **sur cette journÃ©e dÃ©jÃ  existante (retraitement inclus)** avant de poursuivre jusquâ€™Ã  Jâ€“1.
- `get_every_day_flux` : pilote la boucle sur les jours Ã  traiter jusquâ€™Ã  Jâ€“1.
- `get_and_save_daily_flux_rne` : appelle lâ€™API RNE Diff pour une journÃ©e donnÃ©e, Ã©crit les enregistrements JSON ligne Ã  ligne, compresse en `.json.gz`, envoie dans MinIO, gÃ¨re les erreurs et la reprise.
- `ApiRNEClient` : encapsule lâ€™appel Ã  lâ€™API (token, URL de base, retries, adaptation du pageSize).
- `get_last_siren` : rÃ©cupÃ¨re le dernier SIREN traitÃ© dans un fichier flux existant pour reprendre en cas de coupure.
- `send_notification_success_mattermost` / `send_notification_failure_mattermost` : journalisent les rÃ©sultats dans Mattermost.

#### Fonctionnellement
- La derniÃ¨re journÃ©e prÃ©sente dans MinIO est systÃ©matiquement retraitÃ©e pour garantir la complÃ©tude dâ€™un flux qui aurait pu Ãªtre partiellement tÃ©lÃ©chargÃ© lors dâ€™un run prÃ©cÃ©dent.
- DÃ©termine automatiquement Ã  partir du contenu MinIO la premiÃ¨re date de flux Ã  rÃ©cupÃ©rer.
- TÃ©lÃ©charge tous les flux manquants jusquâ€™Ã  Jâ€“1.
- Assure la rÃ©silience en cas dâ€™erreurs API (reprise, rÃ©duction du pageSize, sauvegarde partielle).
- Produit un fichier compressÃ© `.json.gz` par journÃ©e, stockÃ© dans `rne/flux/`.

---

### 5.3 Construction quotidienne de la base consolidÃ©e RNE
- **DAG :** `fill_rne_database`  
  (`workflows/data_pipelines/rne/database/DAG.py`)

#### Ordre des tÃ¢ches
1. `clean_previous_outputs`  
2. `get_start_date`  
3. `create_db`  
4. `get_latest_db`  
5. `process_stock_json_files`  
6. `process_flux_json_files`  
7. `remove_duplicates`  
8. `check_db_count`  
9. `upload_db_to_minio`  
10. `upload_latest_date_rne_minio`  
11. `clean_outputs`  
12. `send_notification_mattermost`

#### Chemin technique global

```
DAG fill_rne_database                              (rne/database/DAG.py)
    â”‚
    â”œâ”€â”€ clean_previous_outputs (BashOperator â€“ DAG.py)
    â”‚       â†’ bash_command: rm -rf ${RNE_DB_TMP_FOLDER} && mkdir -p ${RNE_DB_TMP_FOLDER}
    â”‚
    â”œâ”€â”€ get_start_date (PythonOperator â€“ DAG.py)
    â”‚       â†’ get_start_date_minio(**kwargs)             (rne/database/task_functions.py)
    â”‚               â†’ MinIOClient().get_files(...)       (helpers/minio_helpers.py)
    â”‚                     - source_path: RNE_MINIO_DATA_PATH
    â”‚                     - source_name: RNE_LATEST_DATE_FILE (latest_rne_date.json)
    â”‚                     - dest_path:   RNE_DB_TMP_FOLDER
    â”‚               â†’ ouvre RNE_DB_TMP_FOLDER/latest_rne_date.json
    â”‚               â†’ lit data["latest_date"]
    â”‚               â†’ parse en datetime, reformat YYYY-MM-DD
    â”‚               â†’ ti.xcom_push("start_date", value=start_date)
    â”‚               â†’ en cas de S3Error code "NoSuchKey" :
    â”‚                     ti.xcom_push("start_date", value=None)
    â”‚
    â”œâ”€â”€ create_db (PythonOperator â€“ DAG.py)
    â”‚       â†’ create_db(**kwargs)                        (rne/database/task_functions.py)
    â”‚               â†’ start_date = ti.xcom_pull("start_date", task_ids="get_start_date")
    â”‚               â†’ rne_db_path = create_db_path(start_date)   (task_functions.py)
    â”‚                     - construit: RNE_DB_TMP_FOLDER + f"rne_{start_date}.db"
    â”‚               â†’ ti.xcom_push("rne_db_path", rne_db_path)
    â”‚               â†’ si start_date est non nul:
    â”‚                     return (on ne recrÃ©e pas la DB)
    â”‚               â†’ si fichier rne_db_path existe: os.remove(...)
    â”‚               â†’ connect_to_db(rne_db_path)         (rne/database/db_connexion.py)
    â”‚               â†’ create_tables(cursor)              (rne/database/process_rne.py)
    â”‚               â†’ connection.commit() / close()
    â”‚
    â”œâ”€â”€ get_latest_db (PythonOperator â€“ DAG.py)
    â”‚       â†’ get_latest_db(**kwargs)                    (rne/database/task_functions.py)
    â”‚               â†’ start_date = ti.xcom_pull("start_date", "get_start_date")
    â”‚               â†’ si start_date est non nul:
    â”‚                     previous_start_date = (start_date - 1 jour)
    â”‚                     MinIOClient().get_files(...)   (helpers/minio_helpers.py)
    â”‚                         - source_name: rne_{previous_start_date}.db.gz
    â”‚                         - dest_name:   rne_{start_date}.db.gz
    â”‚                         - paths: RNE_MINIO_DATA_PATH â†’ RNE_DB_TMP_FOLDER
    â”‚                     db_path = RNE_DB_TMP_FOLDER + f"rne_{start_date}.db"
    â”‚                     gzip.open(db_path + ".gz") â†’ open(db_path) (shutil.copyfileobj)
    â”‚                     os.remove(db_path + ".gz")
    â”‚               â†’ get_tables_count(RNE_DB_TMP_FOLDER + f"rne_{start_date}.db")
    â”‚                     (rne/database/process_rne.py)
    â”‚               â†’ logging des counts (UL, siÃ¨ge, dirigeants, immat)
    â”‚
    â”œâ”€â”€ process_stock_json_files (PythonOperator â€“ DAG.py)
    â”‚       â†’ process_stock_json_files(**kwargs)         (rne/database/task_functions.py)
    â”‚               â†’ start_date = ti.xcom_pull("start_date", "get_start_date")
    â”‚               â†’ rne_db_path = ti.xcom_pull("rne_db_path", "create_db")
    â”‚               â†’ minio_client = MinIOClient()
    â”‚               â†’ si start_date est non nul:
    â”‚                     return  (stock traitÃ© uniquement si pas encore de date)
    â”‚               â†’ json_stock_rne_files = minio_client.get_files_from_prefix(
    â”‚                     prefix=RNE_MINIO_STOCK_DATA_PATH
    â”‚                 )
    â”‚               â†’ si liste vide: raise Exception("No RNE stock files found!!!")
    â”‚               â†’ pour chaque file_path dans json_stock_rne_files:
    â”‚                     minio_client.get_files(
    â”‚                         list_files=[{
    â”‚                             "source_path": "",
    â”‚                             "source_name": file_path,
    â”‚                             "dest_path": "",
    â”‚                             "dest_name": file_path,
    â”‚                         }]
    â”‚                     )
    â”‚                     â†’ inject_records_into_db(file_path, rne_db_path, "stock")
    â”‚                           (rne/database/process_rne.py)
    â”‚                     â†’ os.remove(file_path)
    â”‚
    â”œâ”€â”€ process_flux_json_files (PythonOperator â€“ DAG.py)
    â”‚       â†’ process_flux_json_files(**kwargs)          (rne/database/task_functions.py)
    â”‚               â†’ start_date  = ti.xcom_pull("start_date", "get_start_date")
    â”‚               â†’ rne_db_path = ti.xcom_pull("rne_db_path", "create_db")
    â”‚               â†’ minio_client = MinIOClient()
    â”‚               â†’ json_daily_flux_files = minio_client.get_files_from_prefix(
    â”‚                     prefix=RNE_MINIO_FLUX_DATA_PATH
    â”‚                 )
    â”‚               â†’ si liste vide: return
    â”‚               â†’ si start_date est None: start_date = "0000-00-00"
    â”‚               â†’ filtre des fichiers flux par date â‰¥ start_date
    â”‚               â†’ pour chaque file_date retenue:
    â”‚                     minio_client.get_files(
    â”‚                         list_files=[{
    â”‚                             "source_path": RNE_MINIO_FLUX_DATA_PATH,
    â”‚                             "source_name": f"rne_flux_{file_date}.json.gz",
    â”‚                             "dest_path": RNE_DB_TMP_FOLDER,
    â”‚                             "dest_name": f"rne_flux_{file_date}.json.gz",
    â”‚                         }]
    â”‚                     )
    â”‚                     json_path = RNE_DB_TMP_FOLDER + f"rne_flux_{file_date}.json"
    â”‚                     â†’ gzip.open(json_path + ".gz") â†’ open(json_path)
    â”‚                     â†’ os.remove(json_path + ".gz")
    â”‚                     â†’ inject_records_into_db(json_path, rne_db_path, "flux")
    â”‚                           (rne/database/process_rne.py)
    â”‚                     â†’ os.remove(json_path)
    â”‚               â†’ dates = tri des dates "rne_flux_YYYY-MM-DD" dans json_daily_flux_files
    â”‚               â†’ last_date_processed = derniÃ¨re date ou None
    â”‚               â†’ ti.xcom_push("last_date_processed", last_date_processed)
    â”‚
    â”‚       [DÃ©tail interne inject_records_into_db / flux]        (rne/database/process_rne.py)
    â”‚               â†’ pour chaque ligne JSON:
    â”‚                     data = json.loads(line)
    â”‚                     unites_legales_temp = process_records_to_extract_rne_data(
    â”‚                           data, "flux")
    â”‚                     unites_legales += unites_legales_temp
    â”‚                     si len(unites_legales) >= 100000:
    â”‚                           insert_unites_legales_into_db(unites_legales, file_path, db_path)
    â”‚                           unites_legales = []
    â”‚               â†’ flush final via insert_unites_legales_into_db(...)
    â”‚
    â”‚       [DÃ©tail interne mapping / Pydantic]                   (rne/database/process_rne.py)
    â”‚               process_records_to_extract_rne_data(data,"flux")
    â”‚                     â†’ extract_rne_data(entity, "flux")
    â”‚                           â†’ company = entity["company"]
    â”‚                           â†’ rne_company = RNECompany.model_validate(company)
    â”‚                                 (rne/database/rne_model.py â€“ Pydantic)
    â”‚                           â†’ unite_legale = UniteLegale()        (rne/database/ul_model.py)
    â”‚                           â†’ unite_legale = map_rne_company_to_ul(
    â”‚                                 rne_company, unite_legale)      (rne/database/map_rne.py)
    â”‚                           â†’ retourne UniteLegale rempli
    â”‚               insert_unites_legales_into_db(...)
    â”‚                     â†’ connect_to_db(db_path)                   (db_connexion.py)
    â”‚                     â†’ find_and_delete_same_siren(...)          (process_rne.py)
    â”‚                     â†’ INSERT INTO: unite_legale, siege, dirigeant_pp,
    â”‚                                       dirigeant_pm, immatriculation,
    â”‚                                       etablissement, activite
    â”‚                     â†’ commit / close
    â”‚
    â”œâ”€â”€ remove_duplicates (PythonOperator â€“ DAG.py)
    â”‚       â†’ remove_duplicates(**kwargs)                  (rne/database/task_functions.py)
    â”‚               â†’ rne_db_path = ti.xcom_pull("rne_db_path", "create_db")
    â”‚               â†’ connection, cursor = connect_to_db(rne_db_path)
    â”‚               â†’ tables = ["unites_legale","siege","dirigeant_pp",
    â”‚                            "dirigeant_pm","immatriculation"]
    â”‚               â†’ pour chaque table:
    â”‚                     remove_duplicates_from_tables(cursor, table)
    â”‚                           (rne/database/process_rne.py)
    â”‚                             - crÃ©e table temporaire
    â”‚                             - INSERT DISTINCT
    â”‚                             - DROP ancienne table
    â”‚                             - RENAME temp â†’ table
    â”‚               â†’ VACUUM
    â”‚               â†’ commit, close (rollback + close en cas dâ€™exception)
    â”‚
    â”œâ”€â”€ check_db_count (PythonOperator â€“ DAG.py)
    â”‚       â†’ check_db_count(ti, min_*_table_count=...)    (rne/database/task_functions.py)
    â”‚               â†’ rne_db_path = ti.xcom_pull("rne_db_path", "create_db")
    â”‚               â†’ count_ul, count_siege, count_pp, count_pm, count_immat = \
    â”‚                     get_tables_count(rne_db_path)    (rne/database/process_rne.py)
    â”‚               â†’ logging des counts
    â”‚               â†’ si un count < min correspondant:
    â”‚                     raise Exception("Minimum threshold not met...")
    â”‚
    â”œâ”€â”€ upload_db_to_minio (PythonOperator â€“ DAG.py)
    â”‚       â†’ upload_db_to_minio(**kwargs)                 (rne/database/task_functions.py)
    â”‚               â†’ start_date         = ti.xcom_pull("start_date", "get_start_date")
    â”‚               â†’ last_date_processed= ti.xcom_pull("last_date_processed", "process_flux_json_files")
    â”‚               â†’ database_file_path     = RNE_DB_TMP_FOLDER + f"rne_{start_date}.db"
    â”‚               â†’ database_zip_file_path = RNE_DB_TMP_FOLDER + f"rne_{start_date}.db.gz"
    â”‚               â†’ gzip de rne_{start_date}.db â†’ rne_{start_date}.db.gz
    â”‚               â†’ os.remove(database_file_path)
    â”‚               â†’ send_to_minio([...])                  (task_functions.py)
    â”‚                     â†’ MinIOClient().send_files(...)   (helpers/minio_helpers.py)
    â”‚                           - source_name: rne_{start_date}.db.gz
    â”‚                           - dest_name:   rne_{last_date_processed}.db.gz
    â”‚               â†’ os.remove(database_zip_file_path) (si prÃ©sent)
    â”‚
    â”œâ”€â”€ upload_latest_date_rne_minio (PythonOperator â€“ DAG.py)
    â”‚       â†’ upload_latest_date_rne_minio(ti)             (rne/database/task_functions.py)
    â”‚               â†’ last_date_processed = ti.xcom_pull("last_date_processed",
    â”‚                                                    "process_flux_json_files")
    â”‚               â†’ last_date_processed = datetime.strptime(..., "%Y-%m-%d")
    â”‚               â†’ latest_date = (last_date_processed + 1 jour).strftime("%Y-%m-%d")
    â”‚               â†’ Ã©crit RNE_DB_TMP_FOLDER + "latest_rne_date.json"
    â”‚                     {"latest_date": latest_date}
    â”‚               â†’ send_to_minio([...])                  (task_functions.py)
    â”‚                     - source_path: RNE_DB_TMP_FOLDER
    â”‚                     - source_name: RNE_LATEST_DATE_FILE
    â”‚                     - dest_path:   RNE_MINIO_DATA_PATH
    â”‚                     - dest_name:   RNE_LATEST_DATE_FILE
    â”‚
    â”œâ”€â”€ clean_outputs (BashOperator â€“ DAG.py)
    â”‚       â†’ bash_command: rm -rf ${RNE_DB_TMP_FOLDER}
    â”‚
    â””â”€â”€ send_notification_mattermost (PythonOperator â€“ DAG.py)
            â†’ notification_mattermost(ti)                 (rne/database/task_functions.py)
                    â†’ start_date = ti.xcom_pull("start_date", "get_start_date")
                    â†’ last_date_processed = ti.xcom_pull("last_date_processed",
                    â”‚                                     "process_flux_json_files")
                    â†’ send_message("ğŸŸ¢ DonnÃ©es RNE traitÃ©es de {start_date} Ã  {last_date_processed}.")
                          (helpers/mattermost.py)
```

#### Fichiers utilisÃ©s
- `workflows/data_pipelines/rne/database/DAG.py`
- `workflows/data_pipelines/rne/database/task_functions.py`
- `workflows/data_pipelines/rne/database/db_connexion.py`
- `workflows/data_pipelines/rne/database/process_rne.py`
- `workflows/data_pipelines/rne/database/map_rne.py`
- `workflows/data_pipelines/rne/database/rne_model.py`
- `workflows/data_pipelines/rne/database/ul_model.py`

#### Fonctions / classes clÃ©s
- `get_start_date_minio` : lit `latest_rne_date.json` dans MinIO, en extrait `latest_date` et pousse `start_date` en XCom (ou `None` si fichier absent).
- `create_db` / `create_tables` : CrÃ©e la base SQLite et initialise toutes les tables **uniquement lorsque `start_date` est `None` (premier run)**. Lorsque `start_date` est dÃ©fini, la base nâ€™est pas recrÃ©Ã©e : elle est rÃ©cupÃ©rÃ©e par `get_latest_db`, dÃ©compressÃ©e, puis rÃ©utilisÃ©e telle quelle pour lâ€™injection des nouveaux flux.
- `get_latest_db` : tÃ©lÃ©charge la base prÃ©cÃ©dente depuis MinIO (snapshot `rne_<date>.db.gz` de la veille, i.e. `start_date â€“ 1`), la dÃ©compresse sous `rne_<start_date>.db` et permet ainsi une reprise Ã  partir dâ€™un Ã©tat consolidÃ©.
- `process_stock_json_files` : ne sâ€™exÃ©cute que si `start_date` est `None` (premier run). TÃ©lÃ©charge les JSON du stock depuis MinIO, les injecte via `inject_records_into_db` puis supprime les fichiers locaux.
- `process_flux_json_files` : liste les flux `rne/flux`, exclut le flux le plus rÃ©cent, filtre les fichiers Ã  partir de `start_date` (ou de la date la plus ancienne si premiÃ¨re exÃ©cution), dÃ©compresse puis injecte chaque fichier via `inject_records_into_db`, et pousse `last_date_processed` en XCom.
- `inject_records_into_db` : lit un fichier JSON (stock ou flux), transforme les enregistrements RNE en objets UL (via `process_records_to_extract_rne_data` + `map_rne_company_to_ul`) et insÃ¨re dans les tables SQLite.
- `map_rne_company_to_ul` : convertit les modÃ¨les Pydantic de `rne_model.py` en objets UL cibles (`UniteLegale`, `Siege`, `Etablissement`, `Activite`, `DirigeantsPP`, `DirigeantsPM`, `Immatriculation`) dÃ©finis dans `ul_model.py`.
- `remove_duplicates_from_tables` : supprime les doublons sur les tables clÃ©s.
- `check_db_count` : vÃ©rifie un nombre minimum de lignes par table ; en cas dâ€™anomalie, la tÃ¢che Ã©choue.
- `upload_db_to_minio` : compresse le fichier `.db` en `.db.gz`, lâ€™upload dans `rne/database/` sous le nom `rne_<last_date_processed>.db.gz`, supprime le fichier temporaire local.
- `upload_latest_date_rne_minio` : Ã©crit localement `latest_rne_date.json` en mettant `latest_date = last_date_processed + 1`, lâ€™upload dans `rne/database/` puis supprime la copie locale.
- `send_notification_mattermost` : envoie un rÃ©capitulatif des dates traitÃ©es.

#### Fonctionnellement
- RÃ©cupÃ¨re lâ€™Ã©tat antÃ©rieur (base prÃ©cÃ©dente) si on nâ€™est pas au tout premier run.
- Charge le stock complet uniquement une fois (premiÃ¨re exÃ©cution historique).
- Applique les flux manquants pour amener la base Ã  jour jusquâ€™Ã  Jâ€“1.
- Garantit la cohÃ©rence et lâ€™absence de doublons.
- Produit un snapshot datÃ© exploitable (`rne_<date>.db.gz`) et une mÃ©tadonnÃ©e `latest_rne_date.json` qui sert de point de reprise.

---

## 6. SÃ©quence dâ€™exÃ©cution de bout en bout

### 6.1 RÃ©sumÃ© sÃ©quentiel

1. **Acquisition du stock (ponctuel, manuel)**  
   - `get_rne_stock`  
   - TÃ©lÃ©chargement stock RNE â†’ extraction â†’ stockage JSON bruts dans `rne/stock/`.

2. **Acquisition des flux (quotidien Ã  01h)**  
   - `get_flux_rne`  
   - Calcul de la date de dÃ©part â†’ appels API journaliers â†’ fichiers `.json.gz` en `rne/flux/`.

3. **Consolidation de la base (quotidien Ã  02h)**  
   - `fill_rne_database`  
   - RÃ©cupÃ©ration Ã©ventuelle de la base prÃ©cÃ©dente â†’ (stock si premier run) â†’ flux jusquâ€™Ã  Jâ€“1 â†’ consolidation en SQLite â†’ upload du snapshot + mise Ã  jour de la date.

### 6.2 Vue textuelle simplifiÃ©e

STOCK â†’ FLUX â†’ BASE CONSOLIDÃ‰E

- Le stock sert de fondation.
- Les flux rejouent lâ€™histoire jour aprÃ¨s jour.
- La base consolidÃ©e assemble le tout en une vision unique.

---

## 7. Points dâ€™attention

### 7.1 DÃ©pendance Ã  MinIO
- Tous les DAGs lisent et Ã©crivent dans MinIO.
- Une indisponibilitÃ© ou une erreur dâ€™authentification perturbe la chaÃ®ne complÃ¨te.

### 7.2 Gestion des dates (logique double)
- `get_flux_rne` se base sur les fichiers flux prÃ©sents dans MinIO pour dÃ©cider des jours Ã  traiter.
- `fill_rne_database` se base sur `latest_rne_date.json` pour savoir jusquâ€™oÃ¹ les flux ont Ã©tÃ© intÃ©grÃ©s dans la base.
- Une incohÃ©rence entre les deux (suppression manuelle dâ€™un fichier, rollback, etc.) peut nÃ©cessiter une reprise contrÃ´lÃ©e.

### 7.3 Flux J exclu systÃ©matiquement
- La base RNE du jour N reflÃ¨te lâ€™Ã©tat des flux jusquâ€™Ã  Nâ€“1.
- Toute consommation downstream doit garder en tÃªte ce dÃ©calage dâ€™un jour.

### 7.4 Volume et performance
- La base SQLite grandit avec le temps (plus de flux intÃ©grÃ©s).
- Les opÃ©rations de dÃ©duplication et de VACUUM peuvent devenir plus coÃ»teuses.
- Lâ€™upload des `.db.gz` peut augmenter en taille.

### 7.5 Reprise sur erreur
- Si `get_flux_rne` Ã©choue un jour, les flux de cette journÃ©e seront rattrapÃ©s au prochain run (car absents de MinIO).
- Si `fill_rne_database` Ã©choue, `latest_rne_date.json` nâ€™est pas mis Ã  jour ; le run suivant reprendra au mÃªme `start_date`.

---

## 8. TL;DR (rÃ©sumÃ© ultra-court)

- Le pipeline RNE tourne en trois temps :
  1. Stock (initial)  
  2. Flux (quotidien)  
  3. Base consolidÃ©e (quotidienne)

- Tout transite par MinIO (stock, flux, snapshots, mÃ©tadonnÃ©es).
- La base consolidÃ©e `rne_<date>.db.gz` est le produit final, exploitable en interne.
- Le pipeline ignore volontairement le flux du jour (J) pour garantir la complÃ©tude.
- La reprise aprÃ¨s incident est gÃ©rÃ©e par la combinaison :
  - fichiers prÃ©sents en `rne/flux/`
  - `latest_rne_date.json`
