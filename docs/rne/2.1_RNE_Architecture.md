# 2.1 – Architecture du pipeline RNE

## 1. Objectif

Cette section décrit l’architecture fonctionnelle et technique du pipeline RNE.
Elle présente les composants impliqués, les artefacts produits, et le flux général
qui permet de produire quotidiennement une base SQLite consolidée à partir du
stock INPI et des flux RNE.

L’objectif est d’offrir une vue d’ensemble claire du fonctionnement global avant
d’entrer dans le détail de l’acquisition, du parsing ou du mapping.

---

## 2. Vue d’ensemble du pipeline RNE

Le pipeline RNE repose sur trois étapes principales :

1. **Acquisition du stock initial**
   - Téléchargement automatique du ZIP RNE via le DAG `get_rne_stock` et le script `get_stock.sh`.
   - Extraction des fichiers JSON.
   - Dépôt du contenu dans MinIO `rne/stock/`.

2. **Acquisition quotidienne des flux**
   - Appels quotidiens à l’API RNE Diff (SIREN modifiés).
   - Dépôt des fichiers JSON journaliers dans `rne/flux/`.
   - Le flux le plus récent (jour J) est volontairement ignoré lors de la consolidation pour éviter de traiter un fichier potentiellement incomplet : la base produite couvre donc les données jusqu’à **J–1**.

3. **Construction quotidienne d’une base consolidée**
   - Détermination de la date de départ à partir de `latest_rne_date.json` si présent, sinon utilisation d’une date de repli (`RNE_DEFAULT_START_DATE`, initialisée au 2025-04-03).
   - Agrégation du stock (1ʳᵉ exécution uniquement).
   - Application des flux depuis la dernière date traitée (en excluant le flux le plus récent).
   - Parsing + validation + déduplication.
   - Génération d’une base SQLite `rne_<date>.db.gz` déposée dans `rne/database/`.
   - Mise à jour du fichier de suivi `latest_rne_date.json`.

---

## 3. Composants techniques

### 3.1 Orchestrateur Airflow
Airflow gère les trois DAGs du pipeline :
- `get_rne_stock`
- `get_flux_rne`
- `fill_rne_database`

Il pilote :
- la planification (quotidienne pour le flux, 02h pour la base),
- la gestion des dépendances,
- les callbacks de réussite/échec,
- le placement des fichiers dans les répertoires temporaires des DAGs.

### 3.2 Stockage objets : MinIO
MinIO est utilisé pour stocker :
- le stock JSON RNE  
  → `rne/stock/`
- les flux quotidiens  
  → `rne/flux/`
- les bases consolidées SQLite  
  → `rne/database/`
- les métadonnées  
  → `rne/database/latest_rne_date.json`

MinIO constitue la source unique de vérité entre les DAGs.

### 3.3 Base locale SQLite
Lors de l’exécution de `fill_rne_database`, une base SQLite est créée dans le dossier
temporaire du DAG puis compressée en `.gz`.  
Elle contient toutes les tables normalisées du RNE.

### 3.4 Parsing / Validation JSON
Le traitement utilise :
- **Pydantic** pour la validation des schémas source,
- **Python** pour le parsing des structures complexes,
- **Pandas / SQLAlchemy / sqlite3** pour la création et l’insertion dans la base SQLite.

### 3.5 Métadonnées d’avancement
Le fichier `latest_rne_date.json` est utilisé pour déterminer :
- la date de démarrage du traitement,
- la liste des flux à appliquer,
- le nom de la base générée (`rne_<date>.db.gz`).

En l’absence de fichier (premier run), le pipeline utilise une date de repli
(`RNE_DEFAULT_START_DATE`, fixée au 2025-04-03) comme point de départ.

---

## 4. Artefacts produits

Le pipeline produit quatre types d’artefacts principaux :

| Artefact | Description | Emplacement MinIO |
|---------|-------------|-------------------|
| **Stock RNE (JSON)** | Version complète annuelle (ZIP décompressé). | `rne/stock/` |
| **Flux RNE (JSON)** | Modifications quotidiennes liées aux SIREN (flux J, J-1, etc.). | `rne/flux/` |
| **Base consolidée SQLite** | Résultat du traitement stock + flux (jusqu’à J–1). | `rne/database/` |
| **latest_rne_date.json** | Date du dernier flux pleinement traité + règles de reprise. | `rne/database/` |

Ces artefacts suffisent pour exploiter les données RNE de manière autonome.

---

## 5. Processus haute-niveau

### 5.1 Acquisition du stock
- Exécution du DAG `get_rne_stock`.
- Téléchargement automatique du ZIP via `get_stock.sh`.
- Extraction des JSON.
- Dépôt dans MinIO `rne/stock/`.

### 5.2 Acquisition quotidienne des flux
- Exécution quotidienne du DAG `get_flux_rne` (01h).
- Appel API RNE Diff.
- Dépôt automatique des flux dans MinIO `rne/flux/`.

### 5.3 Consolidation quotidienne
- Lecture de la dernière date traitée dans `latest_rne_date.json`  
  → en cas d’absence, utilisation de `RNE_DEFAULT_START_DATE` (2025-04-03).
- Reprise de la base précédente (si existante).
- Chargement du stock (uniquement à l’amorçage).
- Sélection des flux non encore intégrés **en excluant le flux le plus récent**.
- Parsing des JSON et insertion dans les tables.
- Déduplication interne des entités.
- Génération de la base SQLite + compression en `rne_<date>.db.gz`.
- Mise à jour de `latest_rne_date.json` avec la nouvelle date de référence.
- Nettoyage du répertoire temporaire.

### 5.4 Mise à disposition
- Upload de `rne_<date>.db.gz` dans `rne/database/`.
- Mise à jour du fichier `latest_rne_date.json`.
- Les consommateurs internes peuvent ensuite récupérer la base la plus récente depuis MinIO.

---

## 6. Points forts de l’architecture

- **Autonomie totale**  
  Le pipeline RNE fonctionne sans dépendre d’aucune autre source (SIRENE, ETL global, etc.).

- **Versionning naturel**  
  Chaque exécution génère une nouvelle base horodatée, basée sur la dernière date de flux pleinement exploitable.

- **Idempotence et reprise contrôlée**  
  Grâce à `latest_rne_date.json` et à la date de repli `RNE_DEFAULT_START_DATE`, la reprise est stable et déterministe.

- **Protection contre les flux incomplets**  
  Le flux le plus récent est systématiquement ignoré lors de la consolidation, ce qui garantit que la base livrée ne contient pas de jour partiellement chargé.

- **Extensibilité future**  
  Le pipeline peut être enrichi ou fusionné avec SIRENE dans un lot ultérieur sans refondre la partie RNE.
